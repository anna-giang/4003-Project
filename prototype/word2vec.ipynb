{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "# Load text from documents\n",
    "def loadTextFromFile(directory, filenames, docs):\n",
    "    trainingDirectory = CURRENT_PATH + \"/\" + directory\n",
    "    for filename in os.listdir(trainingDirectory):\n",
    "        filenames.append(int(filename[:-4]))  # Removes the .txt from the filename\n",
    "        with open(trainingDirectory + \"/\" + filename, \"r\") as file:\n",
    "            text = file.read()\n",
    "        docs.append(text)\n",
    "\n",
    "\n",
    "diversityInclusion = \"diversityInclusion\"\n",
    "diversityInclusionFilenames = []\n",
    "diversityInclusionDocs = []\n",
    "diversityInclusionTestFilenames = []\n",
    "diversityInclusionTestDocs = []\n",
    "loadTextFromFile(\n",
    "    diversityInclusion + \"/\" + \"training\",\n",
    "    diversityInclusionFilenames,\n",
    "    diversityInclusionDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    diversityInclusion + \"/\" + \"test\",\n",
    "    diversityInclusionTestFilenames,\n",
    "    diversityInclusionTestDocs,\n",
    ")\n",
    "\n",
    "\n",
    "encourageGenders = \"encourageGenders\"\n",
    "encourageGendersFilenames = []\n",
    "encourageGendersDocs = []\n",
    "encourageGendersTestFilenames = []\n",
    "encourageGendersTestDocs = []\n",
    "loadTextFromFile(\n",
    "    encourageGenders + \"/\" + \"training\",\n",
    "    encourageGendersFilenames,\n",
    "    encourageGendersDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    encourageGenders + \"/\" + \"test\",\n",
    "    encourageGendersTestFilenames,\n",
    "    encourageGendersTestDocs,\n",
    ")\n",
    "\n",
    "mentionOrgFeatures = \"mentionOrgFeatures\"\n",
    "mentionOrgFeaturesFilenames = []\n",
    "mentionOrgFeaturesDocs = []\n",
    "mentionOrgFeaturesTestFilenames = []\n",
    "mentionOrgFeaturesTestDocs = []\n",
    "loadTextFromFile(\n",
    "    mentionOrgFeatures + \"/\" + \"training\",\n",
    "    mentionOrgFeaturesFilenames,\n",
    "    mentionOrgFeaturesDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    mentionOrgFeatures + \"/\" + \"test\",\n",
    "    mentionOrgFeaturesTestFilenames,\n",
    "    mentionOrgFeaturesTestDocs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a table with two columns: filename ('id'), and the text from the file (the job ad)\n",
    "\n",
    "diversityInclusionData = pd.DataFrame()\n",
    "diversityInclusionData[\"id\"] = diversityInclusionFilenames\n",
    "diversityInclusionData[\"text\"] = diversityInclusionDocs\n",
    "diversityInclusionTestData = pd.DataFrame()\n",
    "diversityInclusionTestData[\"id\"] = diversityInclusionTestFilenames\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestDocs\n",
    "\n",
    "encourageGendersData = pd.DataFrame()\n",
    "encourageGendersData[\"id\"] = encourageGendersFilenames\n",
    "encourageGendersData[\"text\"] = encourageGendersDocs\n",
    "encourageGendersTestData = pd.DataFrame()\n",
    "encourageGendersTestData[\"id\"] = encourageGendersTestFilenames\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestDocs\n",
    "\n",
    "mentionOrgFeaturesData = pd.DataFrame()\n",
    "mentionOrgFeaturesData[\"id\"] = mentionOrgFeaturesFilenames\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesDocs\n",
    "mentionOrgFeaturesTestData = pd.DataFrame()\n",
    "mentionOrgFeaturesTestData[\"id\"] = mentionOrgFeaturesTestFilenames\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctutation\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Convert to lower\n",
    "    text = text.lower()\n",
    "    # Remove whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "diversityInclusionData[\"text\"] = diversityInclusionData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "\n",
    "encourageGendersTestData[\"text\"] = encourageGendersData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# A function to remove stopwords and short length words (< 2)\n",
    "def remove_stopwords(text):\n",
    "    new = []\n",
    "    for word in text.split():\n",
    "        if word not in stop and len(word) > 1:\n",
    "            new.append(word)\n",
    "\n",
    "    return \" \".join(new)\n",
    "\n",
    "\n",
    "diversityInclusionData[\"text\"] = diversityInclusionData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "\n",
    "encourageGendersData[\"text\"] = encourageGendersData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading pre-labeled target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversityInclusionLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/diversityInclusion.csv\"\n",
    ")\n",
    "diversityInclusionData = pd.merge(diversityInclusionData, diversityInclusionLabels)\n",
    "diversityInclusionTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/diversityInclusionTest.csv\"\n",
    ")\n",
    "diversityInclusionTestData = pd.merge(\n",
    "    diversityInclusionTestData, diversityInclusionTestLabels\n",
    ")\n",
    "\n",
    "\n",
    "encourageGendersLabels = pd.read_csv(CURRENT_PATH + \"/labels/encourageGenders.csv\")\n",
    "encourageGendersData = pd.merge(encourageGendersData, encourageGendersLabels)\n",
    "encourageGendersTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/encourageGendersTest.csv\"\n",
    ")\n",
    "encourageGendersTestData = pd.merge(\n",
    "    encourageGendersTestData, encourageGendersTestLabels\n",
    ")\n",
    "\n",
    "\n",
    "mentionOrgFeaturesLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/mentionOrgFeatures.csv\"\n",
    ")\n",
    "mentionOrgFeaturesData = pd.merge(mentionOrgFeaturesData, mentionOrgFeaturesLabels)\n",
    "mentionOrgFeaturesTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/mentionOrgFeaturesTest.csv\"\n",
    ")\n",
    "mentionOrgFeaturesTestData = pd.merge(\n",
    "    mentionOrgFeaturesTestData, mentionOrgFeaturesTestLabels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec for manually training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(\n",
    "#         window = 10,\n",
    "#         min_count = 2,\n",
    "#         workers = 4\n",
    "# )\n",
    "\n",
    "# model.build_vocab(train['tokens'], progress_per=1000)\n",
    "\n",
    "# model.epochs\n",
    "\n",
    "# model.corpus_count\n",
    "\n",
    "# model.train(train['text'], total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# model.save(\"./word-2-vec.model\")\n",
    "\n",
    "# model.wv.most_similar(\"male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/nvbg7bb11cq2_8ybkhv_9chw0000gn/T/ipykernel_2087/3076838374.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1193514, 100)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# need to download the model from https://nlp.stanford.edu/projects/glove/\n",
    "# then add to directory\n",
    "glove_path = 'glove.twitter.27B.100d.txt'\n",
    "word2vec_output_file = 'glove-100d'+'.word2vec'\n",
    "\n",
    "glove2word2vec(glove_path, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the GloVe model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King:  [-3.7500e-01 -2.7532e-01  1.2489e-01 -9.2143e-02 -4.3104e-01  2.5268e-02\n",
      " -4.1867e-02  1.2848e-01 -7.9363e-02 -1.0011e-01  1.4076e-01  1.0922e-01\n",
      " -3.4546e+00 -6.9851e-01  6.6580e-01  5.1494e-01  4.5912e-01 -2.1957e-01\n",
      "  4.4094e-01 -3.0631e-01  1.2293e-01 -9.9830e-02 -2.5755e-01 -6.1872e-01\n",
      "  1.0613e+00 -9.4278e-01  1.9284e-01 -8.2089e-02  2.7782e-01 -1.8595e-01\n",
      "  2.9140e-02 -3.0870e-01 -3.9870e-01 -4.3038e-01  3.8403e-01  3.3243e-01\n",
      " -1.4446e-01  1.6682e-01  4.2301e-01 -2.6490e-01 -7.8106e-02 -4.6756e-01\n",
      " -3.4039e-01 -1.3690e-01  7.0890e-01 -4.8015e-01  8.9183e-02 -2.3709e-01\n",
      "  7.5124e-01  2.0507e-01 -5.5263e-01 -3.8105e-01 -7.7082e-02  3.6118e-01\n",
      " -8.9840e-01 -5.3537e-01  3.3161e-01 -1.3460e-01 -5.7742e-02  1.9428e-01\n",
      "  1.8008e-01 -4.0697e-01  2.6654e-03 -7.8771e-02 -2.3616e-01 -9.8115e-01\n",
      " -1.6823e-01  1.1459e-01 -2.7011e-01 -2.1435e-02  2.3491e-01 -1.1341e+00\n",
      " -3.3837e-01  1.6548e-01  5.3073e-01 -3.0098e-01 -3.6769e-01  4.2092e-01\n",
      "  1.4201e-01  1.7346e-02  7.8406e-01  3.1441e-01 -1.3188e-01  5.4101e-01\n",
      " -7.1659e-01  2.3164e-01  7.0832e-02 -4.1095e-02  6.3811e-01  3.8808e-03\n",
      "  6.5486e-01 -5.9610e-01 -4.5893e-01 -4.3281e-01  7.2713e-02  6.5028e-01\n",
      "  5.9122e-02  4.6786e-01 -2.3716e-01  2.4995e-02]\n",
      "Most similar word to King + Woman:  [('queen', 0.7052315473556519)]\n"
     ]
    }
   ],
   "source": [
    "#Show a word embedding\n",
    "print('King: ',model.get_vector('king'))\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print('Most similar word to King + Woman: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('inclusion', 0.7886667847633362),\n",
       " ('sustainability', 0.744009256362915),\n",
       " ('equality', 0.7191735506057739),\n",
       " ('empowerment', 0.717718780040741),\n",
       " ('unity', 0.6994752883911133),\n",
       " ('leadership', 0.6993807554244995),\n",
       " ('advocacy', 0.696628749370575),\n",
       " ('innovation', 0.6965843439102173),\n",
       " ('initiative', 0.686913013458252),\n",
       " ('environmental', 0.685417115688324)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar words to diversity\n",
    "model.most_similar('diversity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a class for vectorizing the sentences\n",
    "\n",
    "Source: https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "#     v = self.word_vectors.get_vector('king')\n",
    "#     self.D = v.shape[0]\n",
    "    self.D = model.vector_size\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding each document and splitting into train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of samples with no words found: 0 / 2\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(1) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb#ch0000025?line=40'>41</a>\u001b[0m     rf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb#ch0000025?line=41'>42</a>\u001b[0m     \u001b[39m# print(rf.fit(Xtrain, Ytrain[i]))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb#ch0000025?line=42'>43</a>\u001b[0m     models\u001b[39m.\u001b[39mappend(rf\u001b[39m.\u001b[39;49mfit(Xtrain, Ytrain[i]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb#ch0000025?line=44'>45</a>\u001b[0m predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/codysu/Desktop/FIT4003/4003-Project/prototype/word2vec.ipynb#ch0000025?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=328'>329</a>\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=330'>331</a>\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=331'>332</a>\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=332'>333</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=333'>334</a>\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/ensemble/_forest.py?line=334'>335</a>\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/base.py?line=593'>594</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/base.py?line=594'>595</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/base.py?line=595'>596</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/base.py?line=596'>597</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/base.py?line=598'>599</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:1090\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1069'>1070</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1070'>1071</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1071'>1072</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1073'>1074</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1074'>1075</a>\u001b[0m     X,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1075'>1076</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1086'>1087</a>\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1087'>1088</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1089'>1090</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1091'>1092</a>\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1093'>1094</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:1100\u001b[0m, in \u001b[0;36m_check_y\u001b[0;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1097'>1098</a>\u001b[0m \u001b[39m\"\"\"Isolated part of check_X_y dedicated to y validation\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1098'>1099</a>\u001b[0m \u001b[39mif\u001b[39;00m multi_output:\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1099'>1100</a>\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1100'>1101</a>\u001b[0m         y,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1101'>1102</a>\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1102'>1103</a>\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1103'>1104</a>\u001b[0m         ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1104'>1105</a>\u001b[0m         dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1105'>1106</a>\u001b[0m         input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1106'>1107</a>\u001b[0m         estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1107'>1108</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1108'>1109</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=1109'>1110</a>\u001b[0m     estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:907\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=898'>899</a>\u001b[0m         _assert_all_finite(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=899'>900</a>\u001b[0m             array,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=900'>901</a>\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=901'>902</a>\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=902'>903</a>\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=903'>904</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=905'>906</a>\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=906'>907</a>\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=907'>908</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=908'>909</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=909'>910</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=910'>911</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=911'>912</a>\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=912'>913</a>\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py:325\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=322'>323</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=323'>324</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=324'>325</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=325'>326</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSingleton array \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m cannot be considered a valid collection.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m x\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=326'>327</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=327'>328</a>\u001b[0m     \u001b[39m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=328'>329</a>\u001b[0m     \u001b[39m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/sklearn/utils/validation.py?line=329'>330</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], numbers\u001b[39m.\u001b[39mIntegral):\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(1) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "# labels = ['target1', 'target2', 'target3']\n",
    "\n",
    "# text_vectors = vectorizer.fit_transform(data['text'])    \n",
    "# target_labels = data[labels]\n",
    "# Xtrain, Xtest, Ytrain, Ytest = train_test_split(text_vectors, target_labels, test_size=0.2, random_state = 200) \n",
    "\n",
    "# models = []\n",
    "\n",
    "# for i in range(3):\n",
    "    \n",
    "#     rf = RandomForestClassifier(n_estimators=200)\n",
    "#     models.append(rf.fit(Xtrain, Ytrain[labels[i]]))\n",
    "\n",
    "# predictions = []\n",
    "# for model in models:\n",
    "#     predictions.append(model.predict(Xtest))\n",
    "\n",
    "# for i in range(3):\n",
    "#     print('\\t Classification report for', labels[i], '\\n')\n",
    "#     print(metrics.classification_report(Ytest[labels[i]], predictions[i],  digits=5))\n",
    "# #     plot_confusion_matrix(Ytest[i], predictions[i])\n",
    "# #     plot_roc_curve(Ytest[i], predictions[i])\n",
    "\n",
    "# Ytrain['target1'].value_counts() .......\n",
    "\n",
    "# Splitting data & implementing bag of words\n",
    "\n",
    "        ### MY TEST ###\n",
    "diversityInclusionText = vectorizer.fit_transform(\n",
    "    diversityInclusionData[\"text\"]\n",
    ")\n",
    "diversityInclusionLabels = np.array(diversityInclusionData[diversityInclusion])\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(diversityInclusionText, diversityInclusionLabels, test_size=0.2, random_state = 200) \n",
    "# diversityInclusionTextFeatures = diversityInclusionText.toarray()\n",
    "print(Ytrain[0])\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    # print(rf.fit(Xtrain, Ytrain[i]))\n",
    "    models.append(rf.fit(Xtrain, Ytrain[i]))\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    predictions.append(model.predict(Xtest))\n",
    "\n",
    "for i in range(3):\n",
    "    print('\\t Classification report for', labels[i], '\\n')\n",
    "    print(metrics.classification_report(Ytest[labels[i]], predictions[i],  digits=5))\n",
    "#     plot_confusion_matrix(Ytest[i], predictions[i])\n",
    "#     plot_roc_curve(Ytest[i], predictions[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "diversityInclusionTestText = vectorizer.fit_transform(\n",
    "    diversityInclusionTestData[\"text\"]\n",
    ")\n",
    "diversityInclusionTestLabels = np.array(diversityInclusionTestData[diversityInclusion])\n",
    "diversityInclusionTestTextFeatures = diversityInclusionTestText.toarray()\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "encourageGendersText = vectorizer.fit_transform(encourageGendersData[\"text\"])\n",
    "encourageGendersLabels = np.array(encourageGendersData[encourageGenders])\n",
    "encourageGendersTextFeatures = encourageGendersText.toarray()\n",
    "\n",
    "encourageGendersTestText = vectorizer.fit_transform(\n",
    "    encourageGendersTestData[\"text\"]\n",
    ")\n",
    "encourageGendersTestLabels = np.array(encourageGendersTestData[encourageGenders])\n",
    "encourageGendersTestTextFeatures = encourageGendersTestText.toarray()\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "mentionOrgFeaturesText = vectorizer.fit_transform(\n",
    "    mentionOrgFeaturesData[\"text\"]\n",
    ")\n",
    "mentionOrgFeaturesLabels = np.array(mentionOrgFeaturesData[mentionOrgFeatures])\n",
    "mentionOrgFeaturesTextFeatures = mentionOrgFeaturesText.toarray()\n",
    "\n",
    "mentionOrgFeaturesTestText = vectorizer.fit_transform(\n",
    "    mentionOrgFeaturesTestData[\"text\"]\n",
    ")\n",
    "mentionOrgFeaturesTestLabels = np.array(mentionOrgFeaturesTestData[mentionOrgFeatures])\n",
    "mentionOrgFeaturesTestTextFeatures = mentionOrgFeaturesTestText.toarray()\n",
    "\n",
    "\n",
    "\n",
    "trainingData = [\n",
    "    [diversityInclusionTextFeatures, diversityInclusionLabels],\n",
    "    [encourageGendersTextFeatures, encourageGendersLabels],\n",
    "    [mentionOrgFeaturesTextFeatures, mentionOrgFeaturesLabels],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and evluating a machine learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    models.append(rf.fit(trainingData[i][0], trainingData[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97\n",
       "1    63\n",
       "Name: target1, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain['target1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    114\n",
       "1     46\n",
       "Name: target2, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain['target2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82\n",
       "1    78\n",
       "Name: target3, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain['target3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22\n",
       "1    18\n",
       "Name: target1, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest['target1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    25\n",
       "1    15\n",
       "Name: target2, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest['target2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23\n",
       "0    17\n",
       "Name: target3, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytest['target3'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
