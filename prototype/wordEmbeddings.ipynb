{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "# Load text from documents\n",
    "def loadTextFromFile(directory, filenames, docs):\n",
    "    trainingDirectory = CURRENT_PATH + \"/\" + directory\n",
    "    for filename in os.listdir(trainingDirectory):\n",
    "        filenames.append(int(filename[:-4]))  # Removes the .txt from the filename\n",
    "        with open(trainingDirectory + \"/\" + filename, \"r\") as file:\n",
    "            text = file.read()\n",
    "        docs.append(text)\n",
    "\n",
    "\n",
    "diversityInclusion = \"diversityInclusion\"\n",
    "diversityInclusionFilenames = []\n",
    "diversityInclusionDocs = []\n",
    "diversityInclusionTestFilenames = []\n",
    "diversityInclusionTestDocs = []\n",
    "loadTextFromFile(\n",
    "    diversityInclusion + \"/\" + \"training\",\n",
    "    diversityInclusionFilenames,\n",
    "    diversityInclusionDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    diversityInclusion + \"/\" + \"test\",\n",
    "    diversityInclusionTestFilenames,\n",
    "    diversityInclusionTestDocs,\n",
    ")\n",
    "\n",
    "\n",
    "encourageGenders = \"encourageGenders\"\n",
    "encourageGendersFilenames = []\n",
    "encourageGendersDocs = []\n",
    "encourageGendersTestFilenames = []\n",
    "encourageGendersTestDocs = []\n",
    "loadTextFromFile(\n",
    "    encourageGenders + \"/\" + \"training\",\n",
    "    encourageGendersFilenames,\n",
    "    encourageGendersDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    encourageGenders + \"/\" + \"test\",\n",
    "    encourageGendersTestFilenames,\n",
    "    encourageGendersTestDocs,\n",
    ")\n",
    "\n",
    "mentionOrgFeatures = \"mentionOrgFeatures\"\n",
    "mentionOrgFeaturesFilenames = []\n",
    "mentionOrgFeaturesDocs = []\n",
    "mentionOrgFeaturesTestFilenames = []\n",
    "mentionOrgFeaturesTestDocs = []\n",
    "loadTextFromFile(\n",
    "    mentionOrgFeatures + \"/\" + \"training\",\n",
    "    mentionOrgFeaturesFilenames,\n",
    "    mentionOrgFeaturesDocs,\n",
    ")\n",
    "loadTextFromFile(\n",
    "    mentionOrgFeatures + \"/\" + \"test\",\n",
    "    mentionOrgFeaturesTestFilenames,\n",
    "    mentionOrgFeaturesTestDocs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a table with two columns: filename ('id'), and the text from the file (the job ad)\n",
    "\n",
    "diversityInclusionData = pd.DataFrame()\n",
    "diversityInclusionData[\"id\"] = diversityInclusionFilenames\n",
    "diversityInclusionData[\"text\"] = diversityInclusionDocs\n",
    "diversityInclusionTestData = pd.DataFrame()\n",
    "diversityInclusionTestData[\"id\"] = diversityInclusionTestFilenames\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestDocs\n",
    "\n",
    "encourageGendersData = pd.DataFrame()\n",
    "encourageGendersData[\"id\"] = encourageGendersFilenames\n",
    "encourageGendersData[\"text\"] = encourageGendersDocs\n",
    "encourageGendersTestData = pd.DataFrame()\n",
    "encourageGendersTestData[\"id\"] = encourageGendersTestFilenames\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestDocs\n",
    "\n",
    "mentionOrgFeaturesData = pd.DataFrame()\n",
    "mentionOrgFeaturesData[\"id\"] = mentionOrgFeaturesFilenames\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesDocs\n",
    "mentionOrgFeaturesTestData = pd.DataFrame()\n",
    "mentionOrgFeaturesTestData[\"id\"] = mentionOrgFeaturesTestFilenames\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove punctutation\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # Convert to lower\n",
    "    text = text.lower()\n",
    "    # Remove whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "diversityInclusionData[\"text\"] = diversityInclusionData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "\n",
    "encourageGendersTestData[\"text\"] = encourageGendersData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestData[\"text\"].apply(\n",
    "    lambda x: clean_text(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# A function to remove stopwords and short length words (< 2)\n",
    "def remove_stopwords(text):\n",
    "    new = []\n",
    "    for word in text.split():\n",
    "        if word not in stop and len(word) > 1:\n",
    "            new.append(word)\n",
    "\n",
    "    return \" \".join(new)\n",
    "\n",
    "\n",
    "diversityInclusionData[\"text\"] = diversityInclusionData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "diversityInclusionTestData[\"text\"] = diversityInclusionTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "\n",
    "encourageGendersData[\"text\"] = encourageGendersData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "encourageGendersTestData[\"text\"] = encourageGendersTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "\n",
    "mentionOrgFeaturesData[\"text\"] = mentionOrgFeaturesData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "mentionOrgFeaturesTestData[\"text\"] = mentionOrgFeaturesTestData[\"text\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading pre-labeled target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "diversityInclusionLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/diversityInclusion.csv\"\n",
    ")\n",
    "diversityInclusionData = pd.merge(diversityInclusionData, diversityInclusionLabels)\n",
    "diversityInclusionTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/diversityInclusionTest.csv\"\n",
    ")\n",
    "diversityInclusionTestData = pd.merge(\n",
    "    diversityInclusionTestData, diversityInclusionTestLabels\n",
    ")\n",
    "\n",
    "\n",
    "encourageGendersLabels = pd.read_csv(CURRENT_PATH + \"/labels/encourageGenders.csv\")\n",
    "encourageGendersData = pd.merge(encourageGendersData, encourageGendersLabels)\n",
    "encourageGendersTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/encourageGendersTest.csv\"\n",
    ")\n",
    "encourageGendersTestData = pd.merge(\n",
    "    encourageGendersTestData, encourageGendersTestLabels\n",
    ")\n",
    "\n",
    "\n",
    "mentionOrgFeaturesLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/mentionOrgFeatures.csv\"\n",
    ")\n",
    "mentionOrgFeaturesData = pd.merge(mentionOrgFeaturesData, mentionOrgFeaturesLabels)\n",
    "mentionOrgFeaturesTestLabels = pd.read_csv(\n",
    "    CURRENT_PATH + \"/labels/mentionOrgFeaturesTest.csv\"\n",
    ")\n",
    "mentionOrgFeaturesTestData = pd.merge(\n",
    "    mentionOrgFeaturesTestData, mentionOrgFeaturesTestLabels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using word2vec for manually training embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(\n",
    "#         window = 10,\n",
    "#         min_count = 2,\n",
    "#         workers = 4\n",
    "# )\n",
    "\n",
    "# model.build_vocab(train['tokens'], progress_per=1000)\n",
    "\n",
    "# model.epochs\n",
    "\n",
    "# model.corpus_count\n",
    "\n",
    "# model.train(train['text'], total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# model.save(\"./word-2-vec.model\")\n",
    "\n",
    "# model.wv.most_similar(\"male\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained Glove word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/91/nvbg7bb11cq2_8ybkhv_9chw0000gn/T/ipykernel_41546/4176141125.py:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_path, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1193514, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# need to download the model from https://nlp.stanford.edu/projects/glove/\n",
    "# then add to directory\n",
    "glove_path = 'glove.twitter.27B.100d.txt'\n",
    "word2vec_output_file = 'MY_MODEL'+'.word2vec'\n",
    "\n",
<<<<<<< HEAD
    "glove2word2vec(glove_path, word2vec_output_file)\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "GloVe model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n"
=======
    "glove2word2vec(glove_path, word2vec_output_file)"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Lexvec model\n",
    "# model = KeyedVectors.load_word2vec_format('lexvec.commoncrawl.ngramsubwords.300d.W.pos.vectors', binary=False)\n"
=======
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the GloVe model\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "King:  [-0.099438 -0.03811  -0.02814  -0.086827 -0.058872 -0.074074 -0.089723\n",
      "  0.007647 -0.079409 -0.077722  0.012145 -0.087492  0.125281  0.077101\n",
      " -0.017215  0.011123  0.011952 -0.002687 -0.119687  0.039859  0.037061\n",
      " -0.070333  0.015584 -0.232619 -0.076024 -0.092917 -0.015832  0.024597\n",
      "  0.022748 -0.081143 -0.002227  0.061346  0.185452  0.161941 -0.021561\n",
      "  0.006761  0.012353 -0.044067  0.057281 -0.12141  -0.114887  0.096228\n",
      "  0.107898 -0.173222  0.014622  0.023282  0.026272 -0.203771  0.113475\n",
      "  0.020428  0.05133   0.032807  0.042183 -0.061823  0.072173  0.065524\n",
      " -0.212801  0.071362 -0.080849 -0.019219 -0.018223 -0.255929  0.106209\n",
      " -0.002017  0.083529  0.109361 -0.121694  0.097796  0.112664  0.069995\n",
      "  0.093611 -0.048229  0.094784 -0.035936 -0.063447 -0.056852  0.080355\n",
      " -0.013907 -0.102584  0.033997 -0.072398 -0.046312  0.126431 -0.010575\n",
      " -0.144504  0.159152 -0.024255 -0.002653 -0.074009  0.009565  0.117276\n",
      " -0.007985  0.108225  0.141307  0.12105   0.002244 -0.109612  0.04216\n",
      " -0.003662  0.005441  0.033684 -0.051304 -0.024535 -0.043478 -0.107928\n",
      " -0.006781  0.067716  0.096214  0.088546 -0.00902  -0.085613 -0.189893\n",
      " -0.077919 -0.072262  0.048512 -0.074116 -0.031344  0.060342  0.22336\n",
      " -0.015441  0.210945  0.210324 -0.066394 -0.113293 -0.004934  0.012272\n",
      " -0.158739  0.023449 -0.070985  0.014054 -0.053843  0.094955  0.128091\n",
      " -0.161265  0.09974   0.028061  0.023285 -0.034525 -0.116481  0.18428\n",
      "  0.088241  0.05717   0.003599 -0.036244 -0.16789   0.129092  0.202253\n",
      "  0.050592 -0.118081 -0.024985 -0.007451 -0.020191 -0.006988 -0.033448\n",
      " -0.018563 -0.166033 -0.086745  0.151754  0.085839  0.073976  0.020392\n",
      "  0.121512 -0.13802  -0.069149 -0.227425 -0.07404  -0.078875 -0.085902\n",
      " -0.007233  0.149313  0.002322  0.099813  0.072284  0.123769  0.026011\n",
      "  0.00363   0.03411  -0.034771 -0.008135  0.075067 -0.065548  0.138004\n",
      "  0.096933  0.08235  -0.05773  -0.091836  0.024751 -0.079274 -0.033573\n",
      "  0.028516  0.069782 -0.08894  -0.021416 -0.031882  0.055589  0.128292\n",
      "  0.012657  0.16202  -0.005074 -0.16005  -0.035528  0.08843  -0.034235\n",
      "  0.033733 -0.02986  -0.046487  0.132608 -0.02022  -0.098516  0.212831\n",
      " -0.010373 -0.032101 -0.102831  0.073216 -0.046638  0.0324    0.008005\n",
      " -0.040234  0.184772  0.044332 -0.036239  0.056628  0.029386 -0.051182\n",
      " -0.107216 -0.030754 -0.067714 -0.020499  0.099033  0.136262  0.014374\n",
      " -0.099437  0.241561  0.188455  0.102933 -0.110954  0.022042  0.02328\n",
      "  0.02836   0.0437   -0.128002 -0.167871 -0.031789  0.0954   -0.005811\n",
      "  0.195208 -0.025032  0.106821  0.007605 -0.002155  0.122571  0.080701\n",
      " -0.074807 -0.140991 -0.097639  0.114417  0.163608 -0.070336 -0.015139\n",
      "  0.037212  0.142003 -0.110911  0.05894  -0.159409  0.070283 -0.095244\n",
      " -0.092594  0.005698  0.168796  0.058782  0.07496   0.089776  0.031367\n",
      "  0.217203 -0.10602  -0.01807  -0.012748  0.081134  0.06839   0.048653\n",
      " -0.115832  0.01078   0.00812  -0.04897  -0.06604  -0.018231 -0.046716\n",
      " -0.052852  0.108196 -0.063047 -0.091859 -0.138051 -0.177897 -0.182835\n",
      "  0.018694  0.101715  0.04978   0.082395 -0.103872  0.044244]\n",
      "Most similar word to King + Woman:  [('queen', 0.6833096146583557)]\n"
=======
      "King:  [-3.7500e-01 -2.7532e-01  1.2489e-01 -9.2143e-02 -4.3104e-01  2.5268e-02\n",
      " -4.1867e-02  1.2848e-01 -7.9363e-02 -1.0011e-01  1.4076e-01  1.0922e-01\n",
      " -3.4546e+00 -6.9851e-01  6.6580e-01  5.1494e-01  4.5912e-01 -2.1957e-01\n",
      "  4.4094e-01 -3.0631e-01  1.2293e-01 -9.9830e-02 -2.5755e-01 -6.1872e-01\n",
      "  1.0613e+00 -9.4278e-01  1.9284e-01 -8.2089e-02  2.7782e-01 -1.8595e-01\n",
      "  2.9140e-02 -3.0870e-01 -3.9870e-01 -4.3038e-01  3.8403e-01  3.3243e-01\n",
      " -1.4446e-01  1.6682e-01  4.2301e-01 -2.6490e-01 -7.8106e-02 -4.6756e-01\n",
      " -3.4039e-01 -1.3690e-01  7.0890e-01 -4.8015e-01  8.9183e-02 -2.3709e-01\n",
      "  7.5124e-01  2.0507e-01 -5.5263e-01 -3.8105e-01 -7.7082e-02  3.6118e-01\n",
      " -8.9840e-01 -5.3537e-01  3.3161e-01 -1.3460e-01 -5.7742e-02  1.9428e-01\n",
      "  1.8008e-01 -4.0697e-01  2.6654e-03 -7.8771e-02 -2.3616e-01 -9.8115e-01\n",
      " -1.6823e-01  1.1459e-01 -2.7011e-01 -2.1435e-02  2.3491e-01 -1.1341e+00\n",
      " -3.3837e-01  1.6548e-01  5.3073e-01 -3.0098e-01 -3.6769e-01  4.2092e-01\n",
      "  1.4201e-01  1.7346e-02  7.8406e-01  3.1441e-01 -1.3188e-01  5.4101e-01\n",
      " -7.1659e-01  2.3164e-01  7.0832e-02 -4.1095e-02  6.3811e-01  3.8808e-03\n",
      "  6.5486e-01 -5.9610e-01 -4.5893e-01 -4.3281e-01  7.2713e-02  6.5028e-01\n",
      "  5.9122e-02  4.6786e-01 -2.3716e-01  2.4995e-02]\n",
      "Most similar word to King + Woman:  [('queen', 0.7052315473556519)]\n"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
     ]
    }
   ],
   "source": [
    "#Show a word embedding\n",
    "print('King: ',model.get_vector('king'))\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "\n",
    "print('Most similar word to King + Woman: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "[('richness', 0.6089744567871094),\n",
       " ('Diversity', 0.5902432799339294),\n",
       " ('diverse', 0.5788357853889465),\n",
       " ('inclusiveness', 0.5460476279258728),\n",
       " ('uniqueness', 0.5407953262329102),\n",
       " ('pluralism', 0.5313884019851685),\n",
       " (\"diversity's\", 0.4992821514606476),\n",
       " ('diversities', 0.49747776985168457),\n",
       " (\"'diversity\", 0.4915635585784912),\n",
       " ('heritage', 0.48739439249038696)]"
=======
       "[('inclusion', 0.7886667847633362),\n",
       " ('sustainability', 0.744009256362915),\n",
       " ('equality', 0.7191735506057739),\n",
       " ('empowerment', 0.717718780040741),\n",
       " ('unity', 0.6994752883911133),\n",
       " ('leadership', 0.6993807554244995),\n",
       " ('advocacy', 0.696628749370575),\n",
       " ('innovation', 0.6965843439102173),\n",
       " ('initiative', 0.686913013458252),\n",
       " ('environmental', 0.685417115688324)]"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar words to diversity\n",
    "model.most_similar('diversity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a class for vectorizing the sentences\n",
    "\n",
    "Source: https://edumunozsala.github.io/BlogEms/jupyter/nlp/classification/embeddings/python/2020/08/15/Intro_NLP_WordEmbeddings_Classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "300"
=======
       "100"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecVectorizer:\n",
    "  def __init__(self, model):\n",
    "    print(\"Loading in word vectors...\")\n",
    "    self.word_vectors = model\n",
    "    print(\"Finished loading in word vectors\")\n",
    "\n",
    "  def fit(self, data):\n",
    "    pass\n",
    "\n",
    "  def transform(self, data):\n",
    "    # determine the dimensionality of vectors\n",
    "#     v = self.word_vectors.get_vector('king')\n",
    "#     self.D = v.shape[0]\n",
    "    self.D = model.vector_size\n",
    "\n",
    "    X = np.zeros((len(data), self.D))\n",
    "    n = 0\n",
    "    emptycount = 0\n",
    "    for sentence in data:\n",
    "      tokens = sentence.split()\n",
    "      vecs = []\n",
    "      m = 0\n",
    "      for word in tokens:\n",
    "        try:\n",
    "          # throws KeyError if word not found\n",
    "          vec = self.word_vectors.get_vector(word)\n",
    "          vecs.append(vec)\n",
    "          m += 1\n",
    "        except KeyError:\n",
    "          pass\n",
    "      if len(vecs) > 0:\n",
    "        vecs = np.array(vecs)\n",
    "        X[n] = vecs.mean(axis=0)\n",
    "      else:\n",
    "        emptycount += 1\n",
    "      n += 1\n",
    "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
    "    return X\n",
    "\n",
    "\n",
    "  def fit_transform(self, data):\n",
    "    self.fit(data)\n",
    "    return self.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in word vectors...\n",
      "Finished loading in word vectors\n"
     ]
    }
   ],
   "source": [
    "# Set a word vectorizer\n",
    "vectorizer = Word2VecVectorizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding each document and splitting into train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Numer of samples with no words found: 0 / 7\n",
      "Numer of samples with no words found: 0 / 3\n",
      "Numer of samples with no words found: 0 / 5\n",
      "Numer of samples with no words found: 0 / 4\n",
      "Numer of samples with no words found: 0 / 8\n",
      "Numer of samples with no words found: 0 / 4\n",
      "[[array([[-0.00804881,  0.062328  , -0.0291645 , ..., -0.03117794,\n",
      "        -0.00888256,  0.06725182],\n",
      "       [-0.00181986,  0.05755581, -0.01650376, ..., -0.039441  ,\n",
      "         0.00648553,  0.03903401],\n",
      "       [-0.00280441,  0.05537559, -0.01449772, ..., -0.03154269,\n",
      "        -0.01114962,  0.04632628],\n",
      "       ...,\n",
      "       [ 0.016755  ,  0.04926629, -0.04735929, ..., -0.03480594,\n",
      "        -0.04079629,  0.05039847],\n",
      "       [-0.01481105,  0.03521071, -0.02987215, ..., -0.02515395,\n",
      "         0.00897514,  0.05121523],\n",
      "       [-0.00270411,  0.050753  , -0.01389973, ..., -0.02832008,\n",
      "         0.00645187,  0.01817235]]), array([1, 0, 1, 1, 1, 1, 1])], [array([[ 0.01952   ,  0.0242138 , -0.030083  , ..., -0.0551442 ,\n",
      "        -0.0313702 ,  0.04281221],\n",
      "       [-0.00979958,  0.04543   , -0.02935867, ..., -0.03090175,\n",
      "        -0.00638325,  0.03268158],\n",
      "       [-0.00590109,  0.05285313, -0.02409611, ..., -0.03392447,\n",
      "         0.01767478,  0.01981309],\n",
      "       [ 0.00710258,  0.03888149,  0.00239897, ..., -0.03529732,\n",
      "         0.00370391, -0.01042677],\n",
      "       [ 0.02275917,  0.04400139, -0.01000872, ..., -0.02609294,\n",
      "        -0.01801883,  0.00194856]]), array([1, 1, 0, 1, 1])], [array([[ 7.29751727e-03,  3.47783491e-02, -6.23869011e-03, ...,\n",
      "        -6.39157221e-02, -2.60195527e-02,  1.85706560e-02],\n",
      "       [ 3.24909319e-03,  7.68172890e-02, -2.27582618e-03, ...,\n",
      "        -2.92986427e-02, -6.51319930e-03,  3.92968133e-02],\n",
      "       [-7.40527612e-05,  2.89593153e-02, -7.86363333e-03, ...,\n",
      "        -6.20719939e-02,  1.17483689e-02,  3.90095264e-02],\n",
      "       ...,\n",
      "       [ 1.19745433e-02,  6.33801669e-02,  5.49399992e-03, ...,\n",
      "        -3.59167941e-02,  1.05949985e-02,  5.27834184e-02],\n",
      "       [ 1.25174578e-02,  2.74593327e-02, -1.14512444e-02, ...,\n",
      "        -4.17260751e-02, -1.43548772e-02,  2.87809670e-02],\n",
      "       [-1.63773817e-04,  5.12303822e-02, -1.19562633e-02, ...,\n",
      "        -4.56416607e-02,  5.74908590e-05,  3.97915840e-02]]), array([1, 1, 1, 0, 1, 1, 1, 1])]]\n"
=======
      "Numer of samples with no words found: 0 / 2\n",
      "Numer of samples with no words found: 0 / 2\n",
      "Numer of samples with no words found: 0 / 2\n",
      "Numer of samples with no words found: 0 / 1\n",
      "Numer of samples with no words found: 0 / 2\n",
      "Numer of samples with no words found: 0 / 2\n",
      "[[array([[ 5.39411977e-02,  1.22836009e-02, -2.37528048e-02,\n",
      "         1.37367338e-01,  1.09552123e-01,  1.43396497e-01,\n",
      "         1.93694815e-01, -3.19377959e-01, -2.68050879e-01,\n",
      "        -3.79796207e-01,  4.78779413e-02, -5.60503125e-01,\n",
      "        -2.23064780e+00,  1.66088536e-01,  3.74895893e-02,\n",
      "        -4.16082829e-01, -2.15344787e-01,  6.75205290e-02,\n",
      "         6.40426353e-02, -7.79727101e-02,  1.87661171e-01,\n",
      "         3.07199031e-01, -2.10115120e-01,  2.79550970e-01,\n",
      "        -6.20019078e-01,  9.92291927e-01,  1.57428145e-01,\n",
      "         4.25259948e-01,  2.08072439e-01,  8.45075473e-02,\n",
      "        -7.41784647e-02,  1.31476283e-01, -2.65927196e-01,\n",
      "        -2.62562837e-02,  3.65779489e-01, -2.86918581e-01,\n",
      "         2.16643512e-01,  2.81479239e-01,  1.27343431e-01,\n",
      "        -1.25272200e-01,  2.78770208e-01, -1.64696336e-01,\n",
      "         2.64531136e-01, -1.14138901e-01,  2.65581757e-01,\n",
      "         2.66149968e-01, -1.03990726e-01, -1.74558446e-01,\n",
      "        -1.91286489e-01, -8.99669230e-02,  6.29488051e-01,\n",
      "        -2.40169272e-01, -3.71133953e-01,  3.01544726e-01,\n",
      "        -3.94920439e-01,  3.45895290e-01, -4.52431887e-02,\n",
      "        -4.32060063e-01, -4.55138981e-01, -1.77196860e-01,\n",
      "         1.38274416e-01,  1.88179716e-01,  5.34993336e-02,\n",
      "         4.07874644e-01,  5.79927742e-01,  4.84381080e-01,\n",
      "         3.06901447e-02,  1.16472267e-01, -3.68251130e-02,\n",
      "        -2.48541862e-01,  1.03906766e-01, -2.27847070e-01,\n",
      "        -1.58080030e-02,  1.41544774e-01,  1.57086298e-01,\n",
      "        -5.72047234e-02, -5.71843311e-02, -2.92345852e-01,\n",
      "         1.01651752e-03, -1.24174871e-01,  4.71410692e-01,\n",
      "         2.72578001e-01,  2.37170741e-01,  5.47285862e-02,\n",
      "        -6.61442876e-02, -4.21143591e-01,  1.76641867e-01,\n",
      "        -2.44236827e-01,  3.81316617e-02,  2.10977465e-01,\n",
      "        -2.74173439e-01,  4.68248516e-01,  1.78639770e-01,\n",
      "        -1.76150247e-01,  1.56348437e-01, -1.67602763e-01,\n",
      "         1.43544093e-01, -6.75581321e-02,  1.96411744e-01,\n",
      "        -1.57871231e-01],\n",
      "       [ 5.39411977e-02,  1.22836009e-02, -2.37528048e-02,\n",
      "         1.37367338e-01,  1.09552123e-01,  1.43396497e-01,\n",
      "         1.93694815e-01, -3.19377959e-01, -2.68050879e-01,\n",
      "        -3.79796207e-01,  4.78779413e-02, -5.60503125e-01,\n",
      "        -2.23064780e+00,  1.66088536e-01,  3.74895893e-02,\n",
      "        -4.16082829e-01, -2.15344787e-01,  6.75205290e-02,\n",
      "         6.40426353e-02, -7.79727101e-02,  1.87661171e-01,\n",
      "         3.07199031e-01, -2.10115120e-01,  2.79550970e-01,\n",
      "        -6.20019078e-01,  9.92291927e-01,  1.57428145e-01,\n",
      "         4.25259948e-01,  2.08072439e-01,  8.45075473e-02,\n",
      "        -7.41784647e-02,  1.31476283e-01, -2.65927196e-01,\n",
      "        -2.62562837e-02,  3.65779489e-01, -2.86918581e-01,\n",
      "         2.16643512e-01,  2.81479239e-01,  1.27343431e-01,\n",
      "        -1.25272200e-01,  2.78770208e-01, -1.64696336e-01,\n",
      "         2.64531136e-01, -1.14138901e-01,  2.65581757e-01,\n",
      "         2.66149968e-01, -1.03990726e-01, -1.74558446e-01,\n",
      "        -1.91286489e-01, -8.99669230e-02,  6.29488051e-01,\n",
      "        -2.40169272e-01, -3.71133953e-01,  3.01544726e-01,\n",
      "        -3.94920439e-01,  3.45895290e-01, -4.52431887e-02,\n",
      "        -4.32060063e-01, -4.55138981e-01, -1.77196860e-01,\n",
      "         1.38274416e-01,  1.88179716e-01,  5.34993336e-02,\n",
      "         4.07874644e-01,  5.79927742e-01,  4.84381080e-01,\n",
      "         3.06901447e-02,  1.16472267e-01, -3.68251130e-02,\n",
      "        -2.48541862e-01,  1.03906766e-01, -2.27847070e-01,\n",
      "        -1.58080030e-02,  1.41544774e-01,  1.57086298e-01,\n",
      "        -5.72047234e-02, -5.71843311e-02, -2.92345852e-01,\n",
      "         1.01651752e-03, -1.24174871e-01,  4.71410692e-01,\n",
      "         2.72578001e-01,  2.37170741e-01,  5.47285862e-02,\n",
      "        -6.61442876e-02, -4.21143591e-01,  1.76641867e-01,\n",
      "        -2.44236827e-01,  3.81316617e-02,  2.10977465e-01,\n",
      "        -2.74173439e-01,  4.68248516e-01,  1.78639770e-01,\n",
      "        -1.76150247e-01,  1.56348437e-01, -1.67602763e-01,\n",
      "         1.43544093e-01, -6.75581321e-02,  1.96411744e-01,\n",
      "        -1.57871231e-01]]), array([1, 1])], [array([[ 1.33551836e-01,  1.01343498e-01,  1.21319167e-01,\n",
      "         1.56200379e-01, -2.05296539e-02,  2.73059726e-01,\n",
      "         3.87008995e-01, -2.71024942e-01,  6.63597062e-02,\n",
      "         2.63209455e-02, -3.13845724e-02, -3.71099770e-01,\n",
      "        -3.49175954e+00, -1.02511235e-01, -2.28793547e-01,\n",
      "        -3.78466919e-02, -9.33472663e-02, -6.14157058e-02,\n",
      "        -1.48128822e-01, -6.89073652e-02,  1.97689295e-01,\n",
      "         1.32050052e-01, -6.67164698e-02,  1.85577229e-01,\n",
      "        -1.82458431e-01,  4.15815353e-01,  6.10419624e-02,\n",
      "         2.74170607e-01, -5.03255464e-02, -2.29922965e-01,\n",
      "         5.29071912e-02, -9.35860649e-02, -2.39772528e-01,\n",
      "         6.72840476e-02, -1.04780152e-01, -7.76858404e-02,\n",
      "         5.09003811e-02, -9.99801829e-02, -6.67713508e-02,\n",
      "         2.50046328e-03, -2.03766823e-01,  5.47719598e-02,\n",
      "         2.70678908e-01, -8.24373439e-02,  6.29243106e-02,\n",
      "         7.64712617e-02,  1.07912883e-01, -4.56379820e-03,\n",
      "        -2.40994290e-01,  1.57228902e-01,  4.54685949e-02,\n",
      "         2.57118493e-01, -6.40785694e-02, -9.70601887e-02,\n",
      "         7.17341751e-02,  1.56861708e-01, -3.09009880e-01,\n",
      "        -9.69208851e-02,  9.19955522e-02,  6.56111315e-02,\n",
      "         1.34713724e-01,  2.79594641e-02,  3.81084047e-02,\n",
      "         3.67679410e-02,  2.96591580e-01,  3.94397415e-02,\n",
      "        -5.35502695e-02, -7.49430582e-02, -2.13415042e-01,\n",
      "         3.97995412e-02, -1.89491346e-01,  1.15905106e-01,\n",
      "         4.62102704e-02, -9.77047533e-02,  1.69564232e-01,\n",
      "        -2.40116686e-01,  5.58880195e-02, -2.22527415e-01,\n",
      "        -2.16485500e-01,  1.21086888e-01,  9.53764498e-01,\n",
      "         8.64893049e-02, -8.97642002e-02, -6.17304705e-02,\n",
      "         2.42424443e-01,  2.05866247e-01,  3.98776531e-02,\n",
      "        -2.50043962e-02, -1.28301784e-01,  1.20908031e-02,\n",
      "        -1.33861706e-01,  1.36543170e-01,  4.73864786e-02,\n",
      "         7.97641948e-02, -3.72551754e-02, -5.52920625e-02,\n",
      "        -4.78291437e-02, -9.08445045e-02,  6.97580948e-02,\n",
      "        -1.23805046e-01],\n",
      "       [ 1.33551836e-01,  1.01343498e-01,  1.21319167e-01,\n",
      "         1.56200379e-01, -2.05296539e-02,  2.73059726e-01,\n",
      "         3.87008995e-01, -2.71024942e-01,  6.63597062e-02,\n",
      "         2.63209455e-02, -3.13845724e-02, -3.71099770e-01,\n",
      "        -3.49175954e+00, -1.02511235e-01, -2.28793547e-01,\n",
      "        -3.78466919e-02, -9.33472663e-02, -6.14157058e-02,\n",
      "        -1.48128822e-01, -6.89073652e-02,  1.97689295e-01,\n",
      "         1.32050052e-01, -6.67164698e-02,  1.85577229e-01,\n",
      "        -1.82458431e-01,  4.15815353e-01,  6.10419624e-02,\n",
      "         2.74170607e-01, -5.03255464e-02, -2.29922965e-01,\n",
      "         5.29071912e-02, -9.35860649e-02, -2.39772528e-01,\n",
      "         6.72840476e-02, -1.04780152e-01, -7.76858404e-02,\n",
      "         5.09003811e-02, -9.99801829e-02, -6.67713508e-02,\n",
      "         2.50046328e-03, -2.03766823e-01,  5.47719598e-02,\n",
      "         2.70678908e-01, -8.24373439e-02,  6.29243106e-02,\n",
      "         7.64712617e-02,  1.07912883e-01, -4.56379820e-03,\n",
      "        -2.40994290e-01,  1.57228902e-01,  4.54685949e-02,\n",
      "         2.57118493e-01, -6.40785694e-02, -9.70601887e-02,\n",
      "         7.17341751e-02,  1.56861708e-01, -3.09009880e-01,\n",
      "        -9.69208851e-02,  9.19955522e-02,  6.56111315e-02,\n",
      "         1.34713724e-01,  2.79594641e-02,  3.81084047e-02,\n",
      "         3.67679410e-02,  2.96591580e-01,  3.94397415e-02,\n",
      "        -5.35502695e-02, -7.49430582e-02, -2.13415042e-01,\n",
      "         3.97995412e-02, -1.89491346e-01,  1.15905106e-01,\n",
      "         4.62102704e-02, -9.77047533e-02,  1.69564232e-01,\n",
      "        -2.40116686e-01,  5.58880195e-02, -2.22527415e-01,\n",
      "        -2.16485500e-01,  1.21086888e-01,  9.53764498e-01,\n",
      "         8.64893049e-02, -8.97642002e-02, -6.17304705e-02,\n",
      "         2.42424443e-01,  2.05866247e-01,  3.98776531e-02,\n",
      "        -2.50043962e-02, -1.28301784e-01,  1.20908031e-02,\n",
      "        -1.33861706e-01,  1.36543170e-01,  4.73864786e-02,\n",
      "         7.97641948e-02, -3.72551754e-02, -5.52920625e-02,\n",
      "        -4.78291437e-02, -9.08445045e-02,  6.97580948e-02,\n",
      "        -1.23805046e-01]]), array([1, 1])], [array([[-1.02820923e-03, -8.68701935e-02, -1.11518070e-01,\n",
      "        -6.44820407e-02,  1.74736649e-01, -1.20623857e-01,\n",
      "         2.82696128e-01, -1.04630336e-01,  8.80183578e-02,\n",
      "        -3.95625800e-01, -6.96502626e-02, -1.91331238e-01,\n",
      "        -2.92131424e+00,  2.80743718e-01,  1.00707049e-02,\n",
      "        -5.38973697e-02, -2.85103679e-01, -1.06903380e-02,\n",
      "        -7.85182938e-02, -1.67898208e-01,  5.60009219e-02,\n",
      "         1.34713769e-01, -1.16290182e-01,  9.02498215e-02,\n",
      "        -2.59058356e-01,  5.94354033e-01,  1.09850630e-01,\n",
      "         3.43442112e-01,  9.14137512e-02,  9.22902450e-02,\n",
      "        -1.00080110e-02, -2.49716252e-01, -3.68950397e-01,\n",
      "        -2.77100820e-02,  4.34862912e-01, -1.65673748e-01,\n",
      "         3.26605856e-01,  4.40716803e-01, -3.09306849e-02,\n",
      "        -4.31149341e-02,  8.44855756e-02,  7.46420696e-02,\n",
      "         1.41330481e-01,  4.96498942e-02,  3.31894785e-01,\n",
      "         2.27953479e-01, -9.90590155e-02, -1.13293767e-01,\n",
      "        -2.33855899e-02,  6.13194592e-02,  2.57982641e-01,\n",
      "        -3.11841428e-01, -4.32596833e-01,  2.57963270e-01,\n",
      "        -2.09847882e-01,  4.60886449e-01, -3.37879546e-02,\n",
      "        -1.40000731e-01, -2.09283546e-01, -2.32517943e-01,\n",
      "        -5.55058680e-02,  1.58831730e-01,  2.67769992e-02,\n",
      "         1.99096411e-01,  3.90789241e-01,  2.00812608e-01,\n",
      "         1.05614096e-01,  1.81295425e-02, -2.22063977e-02,\n",
      "        -4.10071388e-02,  1.06621690e-01,  5.25553674e-02,\n",
      "         1.72034010e-01,  3.69414151e-01,  2.63316065e-01,\n",
      "         1.91093329e-02, -6.65123165e-02, -5.39886206e-02,\n",
      "         7.47627467e-02, -1.79341912e-01,  1.02002645e+00,\n",
      "         7.95561075e-03,  1.18939012e-01,  1.75741799e-02,\n",
      "         1.11537039e-01, -1.12485543e-01,  9.21777040e-02,\n",
      "         7.77843595e-02,  3.61587591e-02,  1.19275540e-01,\n",
      "        -2.91352212e-01,  1.21620938e-01,  1.34716049e-01,\n",
      "        -4.28150706e-02,  2.35584348e-01, -6.51525930e-02,\n",
      "         5.87077849e-02, -7.85689652e-02,  4.85185981e-02,\n",
      "        -1.21557508e-02],\n",
      "       [ 8.53005871e-02, -8.44774544e-02,  3.43640335e-02,\n",
      "        -1.06247149e-01,  2.34988183e-02,  1.48735061e-01,\n",
      "         2.41258904e-01, -1.83843702e-01, -1.51658922e-01,\n",
      "        -3.12924296e-01, -2.35625040e-02, -3.23546857e-01,\n",
      "        -3.08432150e+00,  1.88708797e-01,  1.17159374e-02,\n",
      "        -1.71153143e-01, -2.20156401e-01, -7.77097791e-02,\n",
      "        -1.19259387e-01, -1.59809038e-01,  2.59886265e-01,\n",
      "         1.12746507e-01, -6.44921660e-02,  8.00373629e-02,\n",
      "        -3.89699578e-01,  7.48377144e-01, -3.44016589e-02,\n",
      "         2.95345008e-01,  8.03908482e-02,  4.65477891e-02,\n",
      "        -5.21096475e-02, -1.75230727e-01, -2.53749043e-01,\n",
      "         3.45923118e-02,  3.96749377e-01, -4.03028131e-02,\n",
      "         3.27556789e-01,  2.01623082e-01, -1.20626278e-01,\n",
      "        -7.19972476e-02, -3.90336812e-02, -1.58938486e-02,\n",
      "         1.58326909e-01, -8.97815153e-02,  2.47899115e-01,\n",
      "         1.64299965e-01, -1.04437605e-01, -2.66553592e-02,\n",
      "        -1.33803755e-01,  1.78183675e-01,  2.61492848e-01,\n",
      "        -1.89156473e-01, -2.09965706e-01,  2.27149799e-01,\n",
      "        -1.31255418e-01,  2.48789757e-01, -1.65936723e-01,\n",
      "        -2.32151553e-01, -5.37365824e-02, -2.27130085e-01,\n",
      "         1.04124029e-03,  7.51789883e-02, -1.73047185e-02,\n",
      "         7.45282173e-02,  4.22054529e-01,  1.44271120e-01,\n",
      "         8.88219923e-02, -5.20254225e-02, -1.96896214e-02,\n",
      "         1.22929690e-03,  2.13773213e-02,  1.76891088e-01,\n",
      "         1.46104217e-01,  3.90730262e-01,  3.66390854e-01,\n",
      "        -1.27817139e-01,  7.06486851e-02, -3.47028673e-02,\n",
      "         1.18961921e-02, -2.59513259e-01,  9.66164231e-01,\n",
      "         1.08647561e-02,  2.12885544e-01,  2.24209279e-01,\n",
      "         1.01811364e-01, -9.00081620e-02,  2.19532013e-01,\n",
      "         1.48718074e-01, -9.08461809e-02,  8.79373550e-02,\n",
      "        -7.82078058e-02,  2.12908566e-01,  1.76270992e-01,\n",
      "        -1.52992653e-02,  1.09392382e-01, -1.01473168e-01,\n",
      "         8.37145150e-02, -2.26662774e-02,  4.29520383e-02,\n",
      "        -2.00557709e-01]]), array([1, 1])]]\n"
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
     ]
    }
   ],
   "source": [
    "\n",
    "diversityInclusionText = vectorizer.fit_transform(diversityInclusionData[\"text\"])\n",
    "diversityInclusionLabels = np.array(diversityInclusionData[diversityInclusion])\n",
    "diversityInclusionTextFeatures = diversityInclusionText\n",
    "\n",
    "\n",
    "diversityInclusionTestText = vectorizer.fit_transform(diversityInclusionTestData[\"text\"])\n",
    "diversityInclusionTestLabels = np.array(diversityInclusionTestData[diversityInclusion])\n",
    "diversityInclusionTestTextFeatures = diversityInclusionTestText\n",
    "\n",
    "\n",
    "encourageGendersText = vectorizer.fit_transform(encourageGendersData[\"text\"])\n",
    "encourageGendersLabels = np.array(encourageGendersData[encourageGenders])\n",
    "encourageGendersTextFeatures = encourageGendersText #do we need to make it into an array from numpy\n",
    "\n",
    "encourageGendersTestText = vectorizer.fit_transform(\n",
    "    encourageGendersTestData[\"text\"]\n",
    ")\n",
    "encourageGendersTestLabels = np.array(encourageGendersTestData[encourageGenders])\n",
    "encourageGendersTestTextFeatures = encourageGendersTestText\n",
    "\n",
    "\n",
    "mentionOrgFeaturesText = vectorizer.fit_transform(\n",
    "    mentionOrgFeaturesData[\"text\"]\n",
    ")\n",
    "mentionOrgFeaturesLabels = np.array(mentionOrgFeaturesData[mentionOrgFeatures])\n",
    "mentionOrgFeaturesTextFeatures = mentionOrgFeaturesText\n",
    "\n",
    "mentionOrgFeaturesTestText = vectorizer.fit_transform(\n",
    "    mentionOrgFeaturesTestData[\"text\"]\n",
    ")\n",
    "mentionOrgFeaturesTestLabels = np.array(mentionOrgFeaturesTestData[mentionOrgFeatures])\n",
    "mentionOrgFeaturesTestTextFeatures = mentionOrgFeaturesTestText\n",
    "\n",
    "\n",
    "\n",
    "trainingData = [\n",
    "    [diversityInclusionTextFeatures, diversityInclusionLabels],\n",
    "    [encourageGendersTextFeatures, encourageGendersLabels],\n",
    "    [mentionOrgFeaturesTextFeatures, mentionOrgFeaturesLabels],\n",
    "]\n",
    "print(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and evluating a machine learning model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Classification report for diversityInclusion \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
<<<<<<< HEAD
      "           0    1.00000   0.50000   0.66667         2\n",
      "           1    0.50000   1.00000   0.66667         1\n",
      "\n",
      "    accuracy                        0.66667         3\n",
      "   macro avg    0.75000   0.75000   0.66667         3\n",
      "weighted avg    0.83333   0.66667   0.66667         3\n",
=======
      "           0    0.00000   0.00000   0.00000         1\n",
      "           1    0.50000   1.00000   0.66667         1\n",
      "\n",
      "    accuracy                        0.50000         2\n",
      "   macro avg    0.25000   0.50000   0.33333         2\n",
      "weighted avg    0.25000   0.50000   0.33333         2\n",
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
      "\n",
      "-------------------------------------------------------------------------\n",
      "\t Classification report for encourageGenders \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
<<<<<<< HEAD
      "           0    0.00000   0.00000   0.00000         3\n",
      "           1    0.25000   1.00000   0.40000         1\n",
      "\n",
      "    accuracy                        0.25000         4\n",
      "   macro avg    0.12500   0.50000   0.20000         4\n",
      "weighted avg    0.06250   0.25000   0.10000         4\n",
=======
      "           1    1.00000   1.00000   1.00000         1\n",
      "\n",
      "    accuracy                        1.00000         1\n",
      "   macro avg    1.00000   1.00000   1.00000         1\n",
      "weighted avg    1.00000   1.00000   1.00000         1\n",
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
      "\n",
      "-------------------------------------------------------------------------\n",
      "\t Classification report for mentionOrgFeatures \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
<<<<<<< HEAD
      "           0    1.00000   1.00000   1.00000         1\n",
      "           1    1.00000   1.00000   1.00000         3\n",
      "\n",
      "    accuracy                        1.00000         4\n",
      "   macro avg    1.00000   1.00000   1.00000         4\n",
      "weighted avg    1.00000   1.00000   1.00000         4\n",
=======
      "           1    1.00000   1.00000   1.00000         2\n",
      "\n",
      "    accuracy                        1.00000         2\n",
      "   macro avg    1.00000   1.00000   1.00000         2\n",
      "weighted avg    1.00000   1.00000   1.00000         2\n",
>>>>>>> b7335f51bd4b7d9ed2de4617c122d1396fa5d478
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = []\n",
    "\n",
    "for i in range(len(trainingData)):\n",
    "    rf = RandomForestClassifier(n_estimators=200)\n",
    "    models.append(rf.fit(trainingData[i][0], trainingData[i][1])) #words[i][0], labels[i][1]\n",
    "\n",
    "predictions = []\n",
    "predictions.append(models[0].predict(diversityInclusionTestTextFeatures))\n",
    "predictions.append(\n",
    "    models[1].predict(encourageGendersTestTextFeatures)\n",
    ")  # Encourage both genders\n",
    "predictions.append(\n",
    "    models[2].predict(mentionOrgFeaturesTestTextFeatures)\n",
    ")  # Mention of work features\n",
    "\n",
    "# Evaluating precisions\n",
    "print(\"\\t Classification report for\", diversityInclusion, \"\\n\")\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        diversityInclusionTestLabels, predictions[0], digits=5\n",
    "    )\n",
    ")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\t Classification report for\", encourageGenders, \"\\n\")\n",
    "print(\n",
    "    metrics.classification_report(encourageGendersTestLabels, predictions[1], digits=5)\n",
    ")\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\t Classification report for\", mentionOrgFeatures, \"\\n\")\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        mentionOrgFeaturesTestLabels, predictions[2], digits=5\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
